# Генеративный ИИ
## Предыстория
**ИИ** как научное направление появился в середине XX века благодаря работам А.Тьюринга.

### Экспертные системы
Первые десятилетия пытались основываться на символьных вычислениях и логическом программировании. В результате появились **Экспертные системы**\

### Машинное обучение
В 90-е годы произошел сдвиг в сторону машинного обучения.\
Во главе угла встал анализ автоматический данных для выявления закономерностей, классификации и применения вероятностых моделей.

### Глубокое обучение
В XXI веке, благодаря развитию вычислительных мощностей и применения нейросетей пришло время глубокого обучения.
- **Нейросети** - вдохновленные строением мозга.\
Благодаря способности апроксимировать сложные функции и обучаться на больших объемах данных, нейросети стали прекрасным инструментом для моделирования распределений вероятностей.\
Применялись, в том числе, для решения задач 
  - распознавания образов,
  - обработки естественного языки
  - машинного перевода
- **Рекуррентные нейросети** (**RNN**) позволили учитывать контекст и временные зависимости в данных.\
Обрабатывает данные последовательно, сохраняя информацию о предыдущих элементах в скрытом состоянии.\
Широко применялись в генерации теста, музыки.
- **Сверточные нейронные сети** способны извлекать характерные особенности из данных.\
Позволяет генерировать реалистичные изображения.
### Transformer
Архитектура **Transformer** появилась в 2017 году.\
В отличии от RNN обрабатывает данные не последовательно, а основывается на механизме _внимания_ для установления связей между всеми элементами входной последовательности параллельно.\
Механизм внимания позволяет генерировать более конкретный и осмысленный текст, благодаря фокусировке на наиболее важных частях входных данных.\
Она позволила 
- эффективно обрабатывать длинные последовательности данных,
- улавливать сложные взаимосвязи между элементами
Трансформеры легли в основу мощных языковых моделей (типа GPT, BERT, T5), которые легли в основу **генеративного ИИ**

#### Ключевые компоненты
- **Self-Attention** (**самовнимание**) - позволяет взвешивать важность частей входной последовательности, вычислять взамосвязи, что позволяет учитывать контекст при генерации текста (к примеру)
- **Multi-Head Attention** (**многоголовое внимание**) - расширение самовнимание, используя несколько "голов, фокусирующихся на различных аспектах входных данных
- **Encoder** преобразует входную последовательность в векторное представление
- **Decoder** генерирует выходную последовательность на основе векторного представления

## Отличия между предиктивным и генеративным ИИ
- **Предиктивный ИИ** (дискриминаторный) предназначен для построения прогноза на базе обучения модели
- **Генеративный ИИ** нацелен на создание новых данных, имитирующих реальные (тест, музыка, изображения)

## Генеративный ИИ
### Основные характеристики
- способность к синтезу\
  (создание новых данных, включая текст, аудио, видео и т.д.)
- обучение на неразмеченных данных
- вероятностный подход
- творческий потенциал

### Примеры приложений
- чат-боты (поддержка, с применением LLM)
- генерация изображений (Midjourney, DALL-E 2)
- Создание текстов (рекламные тексты, статьи, сценарии)
- синтез речи и музыки
- научные исследования (генерация гипотез)

### Различия GPT и BERT
- **GPT** (**Generative Pre-trained Transformer**) - декодерная модель, специализиующаяся на генерации текста.\
Обучается предстазывать следующее слово в тексте.
- **BERT** (**Bidirectional Encoder Representations from Transformers**) - кодировщик, модель, понимающая контекст слов в предложении.\
Предсказывает пропущенные слова, смотрит на слова, как слева, так и справа

## Некоторые архитектуры, альтернативные транформерам
- **VAE** (**вариационные автокодировщики**) состоит из кодировщика и декодировщика.\
Применяется в генерации изображений, обработка шума в изображениях, создании молекул в фармацевтике
- **GAN** (**Генеративно-состязательные сети**)\
Состоят из двух сетей - генератора и дискриминатора.\
Генератор генерирует данные, а дискриминатор пытается отличить их от реальных.\
Обучение основано на состязании
- **LSTM** (**Долговременная краткосрочная память**)\
Попытка решить проблему исчезающего градиента у RMM

## Семейство LLM (Большие языковые модели)
### ChatGPT
Семейство моделей, разработанных OpenAI, основанное на архитектура Трансформеров.\
Обучается на массивных текстовых данных и способны
- генерировать связный и грамматически правильный текст,
- переводить языки,
- писать контент, 
- отвечать на вопросы в информативном ключе

### LLaMA
Семейство моделей от Meta, ориентированное на эффективность и доступность.\
По результатам может конкурировать с GPT-3, но требует меньше ресурсов.\

### Gemini
Семейство мультимодальных моделей, разработанных Google.\
Обучены на огромном массиве данных.\
Способны решать сложные задачи, требующие рассуждения, планирования и понимания контекста.\
Помимо генерации текста, кода, переводов языков, могут понимать содержимое изображений и видео.

### YandexGPT
Специализируется на русскоязычном тексте. Доступна в API Yandex Cloud и используется в продуктах Яндекса.

### GigaChat
Разработан Сбером, тоже фокусируется на русском тексте.\
Обладает возможностями по написанию кода, созданию изображений и пр.\
Доступен через API и интегрирован в сервисы Сбера

## Открытые и закрытые модели
### Закрытые (проприетарные) модели
Разработанны и принадлежат компаниям, которые предоставляют платный доступ к API или реботают через механизм лицензий.\
- GPT-3.5
- GPT-4
- Gemini

### Открытые (Open-Source) модели
Их исходный код и веса доступны публично.\
Любой желающий может использовать, менять и распространять эти модели.\
- LLaMa
- Stable Diffusion
- GPT-J
- SberGPT

### Сравнение закрытых и открытых моделей
| Тип      | Преимущества                                                                                                                                     | Недостатки                                                                                                                                                             |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Закрытые | - Высокая производительность<br/>- Постоянная поддержка и обновления<br/>- Доступ к мощной инфраструктуре через API                              | - Ограниченный доступ<br/>- Стоимость использования<br/>- Зависимость от провайдера<br/>-Ограничения по кастомизации<br/>- Потенциальные проблем с конфиденциальностью |
| Открытые | -Беслпатное использование<br/>- Гибкость в настройке и модификации<br/>- Возможность запуска на своем оборудовании<br/>- Прозрачность и контроль | - Требует значительных ресурсов для обучения и доработки<br/>- Может уступать в производтельности коммерческим<br/>- Меньше гарантий стабильности и поддержки          |

### Сервисы для работы с закрытыми и открытыми моделями
- **Google AI Studio**
  - разработана Google и предоставляет облачную среду для работы с моделями, включая генеративные
  - предоставляет беслпатный доступ к вычислительным ресурсам, предобученным моделям и наборам данных
  - интеграрован с другими сервисами Google
- **Hugging Face**
  - оринетирован на open-source модели в области обработки естественного языка и генеративного ИИ
  - предоставляет
    - *Model Hub* с тычячами предобученных моделей
    - *Datasets Hb* с большим количеством наборов данных
  - Для возможности работать без локального развертывания, предоставляет *Inference API*

## Локальные модели или доступ через API
### Локальные модели
Запуск моделей (вроде LLaMa) локально или на своем облаке, предоставляет полный контроль над моделью и данными.\
Это преимущество критично для работы с конфиденциальными данными.\
Однако, это требует определенных мощностей оборудования.

### API-решения
Упрощает интеграцию в приложения и сервисы, снижает порог входа для использования.\
Однако, появляется зависимость от провайдера и вызывает опасения в безопасности данных при использовании.

### Сравнение локальных моделей и моделей через API
| Тип       | Преимущества                                                                                                  | Недостатки                                                                                                                                          |
|-----------|---------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| Локальные | - Контроль над данными и моделью<br/>- Гибкость настройки<br/>- Отсутствие зависимости от внешних провайдеров | - Высокие начальные затраты на инфраструктуру<br/>- Требуется техническая экспертиза для обслуживания                                               |
| API       | - Простота использования и интеграции<br/>- Низкий порог входа<br/>- Мастабируемость                          | - Зависимость от провайдера<br/>- Расходы на использование<br/>- Потенциальные проблемы с конфиденциальностью<br/>- Ограниченная гибкость настройки |

## Дообуччение моделей
### Перенос обучения (transfer learning)
Позволяет использовать в качестве основы предобученную модель и адаптировать под свою специфику вместо обучения с нуля

### Тонкая настройка (Fine-Tuning)
Дообучение, позволяющее адаптировать модель к особенностям новых данных и повысить ее точность и эффективность\
Например, дообучить GPT-3 на наборе медицинских текстов, чтобы она генерировала более точные медицинские отчеты

### Типы моделей в контексте обучения
- **Фундаментальные** (LLaMa, GPT-3) - обучены на массивных наборах данных, без специфической настройки на конкретные задачи
  - имеют широкий спектр знаний
  - могут быть неэффективными для узкоспециализированных задач
- **Instructed-версии** (ChatGPT, InstructGPT) - дополнительно обучаются на наборах данных, содержащих инструкции и при выполнении различных задач
  - позволяет лучше понимать инструкции пользователей
  - позволяет генерировать более релевантные ответы
- **Fine-tuned версии** - дополнительно обучаются на специализированных наборах данных для конкретной области или задачи
  - позволяет достигать высокой точности и эффективности в решении узкоспециализированных задач

### Затраты на обучение
Обучение с нуля может составлять сотни тысяч GPU-часов, что влечет за собой сотни миллионов долларов.\
Полный Fine-Tuning LLM тоже требует значительныз ресурсов, хотя и меньших, чем обучение с нуля. от десятков до сотен тысяч долларов.\

#### LoRA (Low-Rank Adaptation)
Метод Fine-Tuning, который позволяет значительно снизить затраты на вычисления и память, изменяя только часть параметров модели.\
На порядок дешевле, чем полный Fine-Tuning

### Ограничения обучения LLM
- Высокие вычислительные затраты\
В связи с чем, позволить себе такое обучение могут только крупные компании
- Зависимость от боьльших и разнообразных данных\
Для хороших результатов нужны массивные, разнообразные и качественные данныие
- Преблема с интерпретируемостью\
Отладку и анализ затрудняет сложность понимания того, как они принимают решения и генерируют данные
- Этические аспекты\
модели могут генерировать фейковый контент, нарушать авторские права

### Обучение не LLM, а систем, основанных на LLM
Обучение LLM, в большинстве кейсов, экономически неэффективно.\
Более практический подход - обучение систем на основанных на LLM, используя предобученные модели.\
Могут использоваться техники:
- **Prompt Engineering** - разработка эффективных подсказок (prompts) для получения желаемых результатов от LLM
- **RAG (Retrieval Augmented Generation)** - дополнение подсказок информацией из внешних источников данных для повышения точности и релевантности ответов

## Курсы для дальнейшего погружения
- [Бесплатный курс «YandexGPT для начинающих»](https://start.practicum.yandex/yandexgpt-beginner/)
- [Работа с LLM GigaChat](https://courses.sberuniversity.ru/llm-gigachat/)
- [Уроки "Made With ML"](https://github.com/GokuMohandas/Made-With-ML)
- [Prompt engineering 101](https://habr.com/ru/companies/X5Tech/articles/827878/)
- [ChatGPT Prompt Engineering for Developers](https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/dfbds/introduction)
- [Начало работы с Mistral](https://www.deeplearning.ai/short-courses/getting-started-with-mistral/)